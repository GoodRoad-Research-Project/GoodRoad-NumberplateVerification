import onnxruntime as ort
import numpy as np
from PIL import Image, ImageDraw, ImageOps, ImageEnhance
import io
import easyocr
from collections import Counter
from ultralytics import YOLO
from app.config import MODEL_PATH_V9, MODEL_PATH_V8, CONFIDENCE_THRESHOLD
import cv2  # Needs: pip install opencv-python
import re

class AdvancedYOLOService:
    def __init__(self):
        self.session_v9 = None
        self.model_v8 = None
        
        print("üöÄ Initializing Final Hybrid Engine (Otsu + Regex)...")
        # Optimized for English text detection
        self.reader = easyocr.Reader(['en'], gpu=False) 
        self.load_models()

    def load_models(self):


        # --- YOLOv9 ---
        try:
            print(f"üöÄ Loading YOLOv9 from {MODEL_PATH_V9}...")
            self.session_v9 = ort.InferenceSession(MODEL_PATH_V9, providers=["CPUExecutionProvider"])
            self.input_name_v9 = self.session_v9.get_inputs()[0].name
            self.output_names_v9 = [o.name for o in self.session_v9.get_outputs()]
            print("‚úÖ YOLOv9 Loaded")
        except Exception as e:
            print(f"‚ùå YOLOv9 Error: {e}")

        # --- YOLOv8 ---
        try:
            print(f"üöÄ Loading YOLOv8 from {MODEL_PATH_V8}...")
            self.model_v8 = YOLO(MODEL_PATH_V8, task='detect')
            print("‚úÖ YOLOv8 Loaded")
        except Exception as e:
            print(f"‚ùå YOLOv8 Error: {e}")



    def apply_ocr_filters(self, pil_img):
        """
        Applies Otsu's Thresholding to make text 'pop' against the background.
        Crucial for Sri Lankan plates with shadows/glare.
        """
        # Convert PIL to OpenCV format
        img = np.array(pil_img) 
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        
        # 1. Grayscale
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # 2. Gaussian Blur (Remove noise dots)
        blur = cv2.GaussianBlur(gray, (3, 3), 0)
        
        # 3. Otsu's Thresholding (Auto-Binarization)
        # This converts gray blobs into crisp black/white shapes
        _, binary = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        return binary # Returns numpy array for EasyOCR

    def correct_sri_lankan_text(self, text):
        """
        Robust correction for Sri Lankan plates.
        Handles:
        - Provinces: WP, CP, SP, NP, EP, NW, NC, UVA, SG
        - 2-Letter Series (WP GA-1234) & 3-Letter Series (WP CAB-1234)
        - Context-aware character replacement (0 vs O, 8 vs B)
        """
        # 1. Clean and Normalize
        # Remove common delimiters and spaces to get raw sequence
        raw = text.upper().replace("-", "").replace(" ", "").replace(".", "")
        clean = re.sub(r'[^A-Z0-9]', '', raw)
        
        # If too short, probably not a valid plate query
        if len(clean) < 5:
            return text

        # 2. Identify Province
        # Provinces: WP, CP, SP, NP, EP, NW, NC, SG (2 chars) + UVA (3 chars)
        provinces_2 = ["WP", "CP", "SP", "NP", "EP", "NW", "NC", "SG"]
        provinces_3 = ["UVA"]
        
        detected_province = ""
        body = clean

        # Check for 3-letter province first
        if len(clean) >= 7 and clean[:3] in provinces_3:
            detected_province = clean[:3]
            body = clean[3:]
        # Check for 2-letter province
        elif len(clean) >= 6 and clean[:2] in provinces_2:
            detected_province = clean[:2]
            body = clean[2:]
        else:
            # Fallback: Try to fix likely OCR errors in province (e.g., VP -> WP)
            # This is risky without strict confidence, but let's try a few common ones
            prefix2 = clean[:2]
            if prefix2.replace('V', 'W') in provinces_2: # VP -> WP
                detected_province = prefix2.replace('V', 'W')
                body = clean[2:]
            elif len(clean) >= 3 and clean[:3] == "UVA": # Just in case
                 detected_province = "UVA"
                 body = clean[3:]

        # 3. Parse Body (Letters + Numbers)
        # Expected format: letters (2-3) + numbers (4)
        # We assume the LAST 4 characters are numbers.
        
        if len(body) < 4:
            return text # Structure is broken

        numbers_part = body[-4:]
        letters_part = body[:-4]
        
        # 4. Apply Corrections
        
        # Fix Numbers: 0, 1, 8, 5, 2 etc.
        # Map letters that look like numbers to numbers
        num_map = str.maketrans({
            'O': '0', 'Q': '0', 'D': '0',
            'I': '1', 'L': '1', 'T': '1',
            'Z': '2',
            'S': '5',
            'B': '8',
            'A': '4',
            'G': '6' 
        })
        numbers_part = numbers_part.translate(num_map)

        # Fix Letters: O, I, Z, S, B etc.
        # Map numbers that look like letters to letters
        let_map = str.maketrans({
            '0': 'O',
            '1': 'I',
            '2': 'Z',
            '5': 'S',
            '8': 'B',
            '4': 'A',
            '6': 'G'
        })
        letters_part = letters_part.translate(let_map)
        
        # 5. Format Output
        # [PROV] [LETTERS]-[NUMBERS]
        final_plate = ""
        if detected_province:
            final_plate += f"{detected_province} "
        
        if letters_part:
            final_plate += f"{letters_part}-{numbers_part}"
        else:
            final_plate += f"{numbers_part}"
            
        return final_plate.strip()

    def predict(self, image_bytes: bytes):
        img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        
        # 1. Detect
        boxes_v9 = self.run_yolov9_onnx(img)
        boxes_v8 = self.run_yolov8(img)
        all_boxes = boxes_v9 + boxes_v8
        if not all_boxes: return []
        
        # 2. Aggressive NMS (Merge 'Ghost' boxes)
        keep_indices = self.nms(np.array(all_boxes)[:, :4], np.array(all_boxes)[:, 4], 0.30)
        
        final_detections = []
        all_boxes_np = np.array(all_boxes)
        
        # Limit to top 3 detections to avoid garbage
        sorted_indices = sorted(keep_indices, key=lambda i: all_boxes_np[i, 4], reverse=True)[:3]

        for idx in sorted_indices:
            bbox = all_boxes_np[idx, :4].astype(int)
            conf = float(all_boxes_np[idx, 4])
            
            # Crop with percentage padding
            w_box = bbox[2] - bbox[0]
            h_box = bbox[3] - bbox[1]

            padding_x = int(w_box * 0.15)
            padding_y = int(h_box * 0.15)

            x1 = max(0, bbox[0] - padding_x)
            y1 = max(0, bbox[1] - padding_y)
            x2 = min(img.width, bbox[2] + padding_x)
            y2 = min(img.height, bbox[3] + padding_y)
            
            plate_crop = img.crop((x1, y1, x2, y2))
            
            # --- PIPELINE ---
            plate_text = "Unknown"
            
            # 1. Filter (Otsu Thresholding)
            processed_img = self.apply_ocr_filters(plate_crop)
            
            # 3. Read Text
            raw_text = self.read_text_easyocr(processed_img)
            
            # 4. Correct Text (Regex)
            if len(raw_text) > 3:
                plate_text = self.correct_sri_lankan_text(raw_text)

            # Fallback: If "Unknown", try original crop without filters
            if plate_text == "Unknown":
                raw_text_orig = self.read_text_easyocr(np.array(plate_crop))
                if len(raw_text_orig) > 3:
                     plate_text = self.correct_sri_lankan_text(raw_text_orig)

            final_detections.append({
                "class_name": "License Plate",
                "confidence": round(conf, 2),
                "bbox": {"x1": int(bbox[0]), "y1": int(bbox[1]), "x2": int(bbox[2]), "y2": int(bbox[3])},
                "text": plate_text
            })
            
        return final_detections

    def process_video(self, video_path, search_query=None):
        """
        Modified for DEMO:
        1. Runs actual model to find the best real detection.
        2. If 'search_query' is in our hardcoded list, injects it as a fake result.
        3. Returns both.
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return []

        # --- DEMO TRICK DATA ---
        # The specific values you want to "force" detect for the demo
        demo_allowed_inputs = ["cai 7711", "km 7473", "cbo 3401"]
        
        unique_plates = {} # Key: Text, Value: Best Confidence Detection
        frame_count = 0
        
        # 1. RUN THE REAL MODEL (Standard Logic)
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            # Process every 10th frame to save time
            if frame_count % 10 != 0:
                continue
                
            is_success, buffer = cv2.imencode(".jpg", frame)
            if not is_success:
                continue
                
            byte_data = buffer.tobytes()
            detections = self.predict(byte_data)
            
            for det in detections:
                text = det.get("text", "Unknown")
                conf = det.get("confidence", 0)
                
                if text == "Unknown":
                    continue
                    
                # Store only the highest confidence detection for each unique text
                if text not in unique_plates or conf > unique_plates[text]["confidence"]:
                    unique_plates[text] = det

        cap.release()

        # 2. SELECT THE BEST REAL RESULT
        final_results = []
        
        if unique_plates:
            # Sort by confidence and pick the absolute best one
            best_real_detection = sorted(unique_plates.values(), key=lambda x: x['confidence'], reverse=True)[0]
            final_results.append(best_real_detection)

        # 3. EXECUTE THE "DEMO TRICK" (Inject Manual Input)
        if search_query:
            # Normalize input (remove spaces/dashes, make lower) to match easy
            clean_query = search_query.lower().strip()
            
            # Check if the input is one of your "Magic" values
            if clean_query in demo_allowed_inputs:
                
                # Format it nicely (Uppercase, e.g., "cai 7711" -> "CAI-7711")
                # Simple formatter for the demo text
                display_text = clean_query.upper().replace(" ", "-")
                
                # Create a "Fake" Detection Object
                fake_detection = {
                    "class_name": "License Plate",
                    "confidence": 0.99,  # Fake high confidence
                    "bbox": {"x1": 50, "y1": 50, "x2": 250, "y2": 150}, # Dummy bbox (top left corner)
                    "text": display_text,
                    "is_demo_injection": True # Flag (optional, good for debugging)
                }
                
                # Add to results
                final_results.append(fake_detection)

        return final_results

    def read_text_easyocr(self, img_input):
        result = self.reader.readtext(img_input, detail=0)
        return "".join(result)

    # --- Helpers ---
    def run_yolov9_onnx(self, img):
        if not self.session_v9: return []
        img_resized = img.resize((640, 640))
        input_data = np.array(img_resized, dtype=np.float32).transpose(2, 0, 1) / 255.0
        input_data = input_data[None, ...]
        outputs = self.session_v9.run(self.output_names_v9, {self.input_name_v9: input_data})
        predictions = np.squeeze(outputs[0]).T
        boxes = []
        w_scale, h_scale = img.size[0]/640, img.size[1]/640
        for p in predictions:
            if p[4] > CONFIDENCE_THRESHOLD:
                cx, cy, w, h = p[0], p[1], p[2], p[3]
                boxes.append([(cx-w/2)*w_scale, (cy-h/2)*h_scale, (cx+w/2)*w_scale, (cy+h/2)*h_scale, p[4]])
        return boxes

    def run_yolov8(self, img):
        if not self.model_v8: return []
        results = self.model_v8(img, conf=CONFIDENCE_THRESHOLD, verbose=False)
        boxes = []
        for r in results:
            for b in r.boxes:
                c = b.xyxy[0].cpu().numpy()
                boxes.append([c[0], c[1], c[2], c[3], float(b.conf[0])])
        return boxes

    def nms(self, boxes, scores, thresh):
        if len(boxes) == 0: return []
        x1, y1, x2, y2 = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]
        areas = (x2 - x1) * (y2 - y1)
        order = scores.argsort()[::-1]
        keep = []
        while order.size > 0:
            i = order[0]
            keep.append(i)
            xx1 = np.maximum(x1[i], x1[order[1:]])
            yy1 = np.maximum(y1[i], y1[order[1:]])
            xx2 = np.minimum(x2[i], x2[order[1:]])
            yy2 = np.minimum(y2[i], y2[order[1:]])
            w, h = np.maximum(0.0, xx2 - xx1), np.maximum(0.0, yy2 - yy1)
            inter = w * h
            ovr = inter / (areas[i] + areas[order[1:]] - inter)
            inds = np.where(ovr <= thresh)[0]
            order = order[inds + 1]
        return keep

    def draw_detections(self, image_bytes, detections):
        img = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        draw = ImageDraw.Draw(img)
        for det in detections:
            b = det["bbox"]
            text = det.get("text", "")
            draw.rectangle([b["x1"], b["y1"], b["x2"], b["y2"]], outline="cyan", width=4)
            if text:
                draw.rectangle([b["x1"], b["y1"]-25, b["x1"]+150, b["y1"]], fill="cyan")
                draw.text((b["x1"]+5, b["y1"]-20), f"{text} ({det['confidence']})", fill="black")
        output = io.BytesIO()
        img.save(output, format="JPEG")
        return output.getvalue()

detector = AdvancedYOLOService()